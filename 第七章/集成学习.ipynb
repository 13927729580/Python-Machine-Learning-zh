{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 集成学习\n",
    "\n",
    "前一章我们主要学习了怎样调参以及对模型进行评估。这一章，我们就要实际运用这些技巧，继而探索构建集成分类器的不同方法，集成分类器得到的结果通常比单个分类器要好。\n",
    "\n",
    "这一章你将要学习：\n",
    "\n",
    "* 基于投票的预测\n",
    "* 通过可重复采用构建训练集，降低过拟合\n",
    "* 以弱学习器为基础，从错误中学习来构建强学习器\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 集成学习\n",
    "\n",
    "集成学习背后的思想是将不同的分类器进行组合得到一个元分类器，这个元分类器相对于单个分类器拥有更好的泛化性能。比如，假设我们从10位专家那里分别得到了对于某个事件的预测结果，集成学习能够对这10个预测结果进行组合，得到一个更准确的预测结果。\n",
    "\n",
    "后面我们会学到，有不同的方法来创建集成模型，这一节我们先解决一个基本的问题：为什么要用集成学习？她为什么就比单个模型效果要好呢？\n",
    "\n",
    "\n",
    "本书是为初学者打造的，所以集成学习这里我们也只关注最基本的集成方法：投票法(majority voting)。投票法意味着我们在得到最后的预测类别时，看看哪个类别是大多数单分类器都预测的，这里的大多数一般是大于50%。更严格来说，投票法只适用于二分类，当然他很容易就扩展到多分类情况: 多数表决(plurality voting).\n",
    "\n",
    "\n",
    "下图展示了一个投票法的例子，一共10个基本分类器：\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/07/04/577a60abc6aa0.png)\n",
    "\n",
    "\n",
    "我们先用训练集训练m个不同的分类器$(C_{1},...,C_{m})$, 这里的分类器可以是决策树、SVM或者LR等。我们当然也可以用同一种分类器，只不过在训练每一个模型时用不同的参数或者不同的训练集(比如自主采样法)。随机森林就是一个采用这种策略的例子，它由不同的决策树模型构成。这图展示了用投票策略的集成方法步骤：\n",
    "\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/07/04/577a61b502ccf.png)\n",
    "\n",
    "投票策略非常简单，我们收集每个单分类器$c_{j}$的预测类别$\\hat{y}$,将票数最多的$\\hat{y}$作为预测结果：\n",
    "\n",
    "![](http://odw1x7kgr.bkt.clouddn.com/QQ20161018-4.png)\n",
    "\n",
    "以二分类为例，类别class1=-1, class2=+1, 投票预测的过程如下, 把每个单分类器的预测结果相加，如果值大于0，预测结果为正类，否则为负类：\n",
    "\n",
    "![](http://odw1x7kgr.bkt.clouddn.com/QQ20161018-5.png)\n",
    "\n",
    "\n",
    "读到这里，我想大家都有一个疑问：凭啥集成学习就比单分类器效果好？道理很简单(一点点组合数学知识)，假设对于一个二分类问题，有n个单分类器，每个单分类器有相等的错误率$\\epsilon$，并且单分类器之间相互独立，错误率也不相关。 有了这些假设，我们可以计算集成模型的错误概率：\n",
    "\n",
    "![](http://odw1x7kgr.bkt.clouddn.com/QQ20161018-6.png)\n",
    "\n",
    "\n",
    "如果n=11，错误率为0.25，要想集成结果预测错误，至少要有6个单分类器预测结果不正确，错误概率是：\n",
    "\n",
    "\n",
    "![](http://odw1x7kgr.bkt.clouddn.com/QQ20161018-8.png)\n",
    "\n",
    "\n",
    "集成结果错误率才0.034哦，比0.25小太多。继承结果比单分类器好，也是有前提的，就是你这个单分类器的能力不能太差，至少要比随机猜测的结果好一点，至少。\n",
    "\n",
    "从下图可以看出，只要单分类器的表现不太差，集成学习的结果总是要好于单分类器的。\n",
    "\n",
    "![](http://odw1x7kgr.bkt.clouddn.com/QQ20161018-9.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结合不同的分类算法进行投票\n",
    "\n",
    "\n",
    "这一节学习使用sklearn进行投票分类，看一个具体的例子，数据集采用Iris数据集，只使用sepal width和petal length两个维度特征，类别我们也只是用两类：Iris-Versicolor和Iris-Virginica，评判标准使用ROC AUC。\n",
    "\n",
    "![](http://odw1x7kgr.bkt.clouddn.com/QQ20161018-6@2x.png)\n",
    "\n",
    "\n",
    "\n",
    "![](http://odw1x7kgr.bkt.clouddn.com/QQ20161018-7@2x.png)\n",
    "\n",
    "![](http://odw1x7kgr.bkt.clouddn.com/QQ20161018-8@2x.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
