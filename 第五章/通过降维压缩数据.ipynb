{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第五章 通过降维压缩数据\n",
    "\n",
    "在第四章，你学习了使用不同的特征选择方法来降维，除了特征选择，另一种降维的方法是特征抽取(feature extraction)。本章你将会学到三种基本的方法，帮助我们摘要数据集的信息，即将原始的特征空间压缩到低纬度的特征子空间。数据压缩是机器学习中的重要课题，它能帮助我们存储和分析当今时代不断涌现的大数据。本章，我们主要讨论以下几个内容：\n",
    "* 主成分分析（principal component analysis, PCA), 用于无监督数据压缩\n",
    "* 线性判别分析(linear discriminant analysis, LDA), 用于监督降维作为一种监督降维\n",
    "* 通过核PCA进行非线性降维\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA进行无监督降维\n",
    "\n",
    "类似于特征选择，我们可以使用特征抽取来减小数据集中的特征维度。不过，不同于特征选择算法会保留原始特征空间，特征抽取会将原始特征转换/映射到一个新的特征空间。换句话说，特征抽取可以理解为是一种数据压缩的手段，同时保留大部分相关信息(译者注：理解为摘要)。特征抽取是用于提高计算效率的典型手段，另一个好处是也能够减小维度诅咒(curse of dimensionality)，特别是对于没有正则化的模型。\n",
    "\n",
    "PCA(principal component analysis, 主成分分析)是一种被广泛使用的无监督的线性转换技术，主要用于降维。其他领域的应用还包括探索数据分析和股票交易的信号去噪，基因数据分析和基因表达。\n",
    "\n",
    "PCA根据特征之间的相关性帮助我们确定数据中存在的模式。简而言之，PCA的目标是找到高维数据中最大方差的方向，并且将高维数据映射到一个新的子空间，这个子空间的方向不大于原始特征空间。新子空间的正交轴(主成分)可以被解释为原始空间的最大方差方向。下图中$x_{1}, x_{2}$是原始特征轴，$PC1和PC2$是主成分：\n",
    "\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/23/576c92f32c7a7.png)\n",
    "\n",
    "\n",
    "\n",
    "如果使用PCA降维，我们需要构造一个d*k维的转换矩阵$W$，它能将样本向量$x$映射到新的k维度的特征子空间，k<<d:\n",
    "\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/23/576c937fddbb3.png)\n",
    "\n",
    "\n",
    "映射后的子空间，第一主成分包含最大的方差，第二主成分包含次大的方差，以此类推，并且各个主成分方向是正交的(不相关)。\n",
    "\n",
    "PCA方向极其容易受到数据中特征范围影响，所以**在运用PCA前一定要做特征标准化**，这样才能保证每维度特征的重要性等同。\n",
    "\n",
    "\n",
    "再详细讲解PCA的细节之前，我们先介绍 PCA的步骤：\n",
    "\n",
    "\n",
    "* 1 将d维度原始数据标准化。\n",
    "* 2 构建协方差矩阵。\n",
    "* 3 求解协方差矩阵的特征向量和特征值。\n",
    "* 4 选择值最大的k个特征值对应的特征向量，k就是新特征空间的维度，k<<d。\n",
    "* 5 利用k特征向量构建映射矩阵$W$。\n",
    "* 6 将原始d维度的数据集X，通过映射矩阵W转换到k维度的特征子空间。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 聊一聊方差\n",
    "\n",
    "本节，我们会学习PCA中的前四个步骤：标准化数据、构建协方差矩阵、得到特征值和特征向量以及对特征值排序得到排名靠前的特征向量。\n",
    "\n",
    "\n",
    "数据集还是用第四章介绍过的Wine数据集，先将原始Wine分割为训练集和测试集，然后标准化：\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/23/576ca23d2ac91.png)\n",
    "\n",
    "\n",
    "上面的代码完成了PCA的第一步，对数据进行标准化。我们再来看第二步：构建协方差矩阵。协方差矩阵是对称矩阵，d*d维度，其中d是原始数据的特征维度，协方差矩阵的每个元素是两两特征之间的协方差。\n",
    "\n",
    "举个例子，特征$x_{j}和x_{k}$的协方差计算公式如下：\n",
    "\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/23/576ca3247088d.png)\n",
    "\n",
    "其中，$u_{j}和u_{k}$分别是样本中第j维度特征和第k维度特征的平均值。由于我们已经将数据标准化了，所以这里$u_{j}=u_{k}=0$。\n",
    "如果$\\sigma_{jk}>0$意味着j和k这两维度特征值要么同时增加要么同时衰减，反之$\\sigma_{jk}<0$,意味着这两个特征的值变化方向相反。\n",
    "\n",
    "\n",
    "假设数据有三维度特征，协方差矩阵如下：\n",
    "\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/23/576ca4622a37a.png)\n",
    "\n",
    "协方差矩阵的特征向量代表了主成分(最大方差的方向)，对应的特征值决定了特征向量绝对值的大小。在Wine数据集对应的13*13的协方差矩阵，我们会得到13个特征值。\n",
    "\n",
    "\n",
    "如何得到特征值和特征向量呢？会议线性代数课上讲过的内容：\n",
    "\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/23/576ca53ab284d.png)\n",
    "\n",
    "其中，$\\lambda$是特征值，一个常数。我们当然不可能手算特征值，NumPy提供了linalg.eig函数用于得到特征值和特征向量：\n",
    "\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/23/576ca60a1be22.png)\n",
    "\n",
    "\n",
    "我们先使用np.cov方法得到数据的协方差矩阵，然后利用linalg.eig方法计算出特征向量(eigen_vecs)和特征值(eigen_vals)。\n",
    "\n",
    "\n",
    "\n",
    "由于我们的目的是数据压缩，即降维，所以我们只将那些包含最多信息(方差)的特征向量(主成分)拿出来。什么样的特征向量包含的信息最多呢？这就要看特征值了，因为特征值定义了特征向量的大小，我们先对特征值进行降序排序，前k个特征值对应的特征向量就是我们要找的主成分。\n",
    "\n",
    "\n",
    "我们再学一个概念：方差解释率(variance explained ration)。一个特征值的方差解释率就是次特征值在特征值总和的占比：\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/23/576ca832336b3.png)\n",
    "\n",
    "\n",
    "\n",
    "利用NumPy提供的cumsum函数，我们可以计算累计解释方差和：\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/23/576caa5672366.png)\n",
    "\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/23/576caa772eaa7.png)\n",
    "\n",
    "\n",
    "从上面的结果图我们可以看到第一个主成分占了近40%的方差(信息)，前两个主成分占了60%的方差。\n",
    "\n",
    "\n",
    "很多同学看到这里，可能将方差解释率和第四章讲到的用随机森林评估特征重要性联系起来，二者还是有很大区别的，PCA是一种无监督方法，在整个计算过程中我们都没有用到类别信息！随机森林是监督模型，建模时用到了类别信息。\n",
    "\n",
    "方差的物理含义是对值沿着特征轴的传播进行度量。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征转换\n",
    "\n",
    "在得到特征向量后，接下来我们就可以对原始特征进行转换了。本节我们先对特征值进行降序排序，然后用特征向量构建映射矩阵，最后用映射矩阵将原始数据映射到低维度特征子空间。\n",
    "\n",
    "先对特征值排序：\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/23/576caccb7e3dd.png)\n",
    "\n",
    "接下来，我们选择最大的两个特征值对应的特征向量，这里只用两个特征向量是为了下面画图方便，在实际运用PCA时，到底选择几个特征向量，要考虑到计算效率和分类器的表现两个方面(译者注：常用的选择方式是特征值子集要包含90%方差)：\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/23/576cada4e1218.png)\n",
    "\n",
    "\n",
    "现在，我们创建了一个13*2的的映射矩阵W。然后对样本(1*13维度)进行映射，就能得到2维度的新样本：\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/23/576cae20c2eac.png)\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/23/576cae452595c.png)\n",
    "\n",
    "\n",
    "直接对原始数据(124*13)映射：\n",
    "\n",
    "\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/23/576caea74df5e.png)\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576cd63fd542e.png)\n",
    "\n",
    "\n",
    "\n",
    "特征维度降到2维度后，我们就可以用散点图将数据可视化出来了：\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576cd777445bb.png)\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576cd79c21087.png)\n",
    "\n",
    "\n",
    "从上图可以看到，数据在x轴(第一主成分)上要比y轴(第二主成分)分布更广，这也符合方差解释率的结果。数据降维后，直觉上使用线性分类器就能够将数据分类。\n",
    "\n",
    "\n",
    "### scikit-learn中的PCA\n",
    "\n",
    "上一小节我们详细讨论了PCA的步骤，在实际应用时，一般不会使用自己实现，而是直接调用sklearn中的PCA类，PCA类是另一个transformer类：我们先用训练集训练模型参数，然后统一应用于训练集和测试集。\n",
    "\n",
    "\n",
    "下面我们就是用sklearn中的PCA类对Wine数据降维，然后调用逻辑斯蒂回归模型分类，最后将决策界可视化出来：\n",
    "\n",
    "\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576cdb747acfa.png)\n",
    "\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576cdb888be31.png)\n",
    "\n",
    "\n",
    "\n",
    "执行上面的代码，我们应该得到如下的决策界：\n",
    "\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576cdbb2bd6f8.png)\n",
    "\n",
    "\n",
    "如果你仔细观察我们自己实现的PCA得到的散点图和调用sklearn中PCA得到的散点图，两幅图看起来是镜像关系！这是由于NumPy和sklearn求解特征向量时计算的差异，如果你实在看不惯，只需要将其中一个得到的特征向量*(-1)即可。还要注意特征向量一般都要归一化。\n",
    "\n",
    "我们再看看决策界在测试集的分类效果：\n",
    "\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576cdd9177ed5.png)\n",
    "\n",
    "\n",
    "啊哈！测试集仅有一个样本被错误分类，效果很棒。\n",
    "\n",
    "\n",
    "怎样用sklearn的PCA得到每个主成分(不是特征)的方差解释率呢？很简单，初始化PCA时，n_components设置为None，然后通过explained_variance_ratio_属性得到每个主成分的方差解释率：\n",
    "\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576cde8541c6e.png)\n",
    "\n",
    "\n",
    "此时n_components=None, 我们并没有做降维。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA进行监督数据压缩\n",
    "\n",
    "LDA(linear discriminant analysis, 线性判别分析)是另一种用于特征抽取的技术，它可以提高计算效率，对于非正则模型也能减小过拟合。\n",
    "\n",
    "虽然LDA的很多概念和PCA很像，但他俩的目标不同，PCA目标是找到正交的主成分同时保持数据集的最大方差，LDA的目标是为每个类单独优化，得到各个类的最优特征子集。PCA和LDA都是线性转换技术，用于数据压缩，前者是无监算法，后者是监督算法。看到监督两个字，可能你会认为对于分类任务，LDA要比PCA效果更好，但实际却不是这样，在某些分类任务情境下，用PCA预处理数据得到的结果要比LDA号，比如，如果每个类含有的样本比较少。\n",
    "\n",
    "下图画出了对于二分类问题，LDA的一些概念：\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576ce4747d450.png)\n",
    "\n",
    "x轴的线性判别(LD 1),很好地将两个正态分布的类别数据分离。虽然y轴的线性判别(LD 2)捕捉了数据集中的大量方差，但却不是一个好的线性判别，因为它没有捕捉任何与类别判别相关的信息。\n",
    "\n",
    "\n",
    "\n",
    "LDA 的一个假设是数据服从正态分布。同时，我们也假设各个类含有相同的协方差矩阵，每个特征都统计独立。即使真实数据可能不服从上面的某几个假设，但LDA依然具有很好的表现。\n",
    "\n",
    "\n",
    "在学习LDA内部原理前，我们先将它的几大步骤列出来：\n",
    "* 1. 将d维度原始数据进行标准化.\n",
    "* 2. 对每一个类，计算d维度的平均向量.\n",
    "* 3. 构建类间(between-class)散点矩阵$S_{B}$和类内(within-class)散点矩阵$S_{W}$.\n",
    "* 4. 计算矩阵$S_{W}^{-1}S_{B}$的特征向量和特征值.\n",
    "* 5. 选择值最大的前k个特征值对应的特征向量，构建d*d维度的转换矩阵$W$,每一个特征向量是$W$的一列.\n",
    "* 6. 使用矩阵$W$将原始数据集映射到新的特征子空间.\n",
    "\n",
    "**Note** 我们在应用LDA时做出的假设是：特征服从正态分布并且彼此独立，每个类的协方差矩阵都相同。现实中的数据当然不可能真的全部服从这些假设，但是不用担心，即使某一个甚至多个假设不成立，LDA也有不俗的表现.(R.O.Duda, P.E. Hart, and D.G.Stork. *Pattern Classification*. 2nd.Edition. New York, 2001).\n",
    "\n",
    "### 计算散点矩阵\n",
    "\n",
    "数据标准化前面已经说过了，这里就不再讲了，我们说一下如何计算平均向量，然后用这些平均向量分别构建类内散点矩阵和类间散点矩阵。\n",
    "\n",
    "每一个平均向量$m_{i}$存储了类别i的样本的平均特征值$u_{m}$:\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576cf81b58b4d.png)\n",
    "\n",
    "Wine数据集有三个类，每一个的平均向量：\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576cf85702d4d.png)\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576cf94e759d4.png)\n",
    "\n",
    "\n",
    "有了平均向量，我们就可以计算类内散点矩阵$S_{W}$:\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576cf97a62714.png)\n",
    "\n",
    "$S_{W}$就是每个类的散点矩阵$S_{i}$总和。\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576cf998c8d6b.png)\n",
    "\n",
    "代码：\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576cfa97f09ee.png)\n",
    "\n",
    "\n",
    "\n",
    "当我们计算散点矩阵时，我们做出的假设是训练集中的类别时均匀分布的。实际情况往往并不是这样，比如我们将Wine训练集各个类别个数打印出来：\n",
    "\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576cfb31c266f.png)\n",
    "\n",
    "所以在得到每个类别的散点矩阵$S_{i}$后，我们要将其缩放，然后再相加得到$S_{W}$。如果我们将散点矩阵$S_{i}$除以各个类别内样本数$N_{i}$,我们实际上是在计算协方差矩阵$\\Sigma_{i}$. **协方差矩阵是散点矩阵的归一化结果**：\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576cfbe871c14.png)\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576cfc82cd01f.png)\n",
    "\n",
    "\n",
    "得到缩放后的类内散点矩阵后，我们接下来计算类间散点矩阵$S_{B}$:\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576cfcd0b45f6.png)\n",
    "\n",
    "其中，$m$是整个训练集所有样本的特征平均值.\n",
    "\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576d213f4cc3b.png)\n",
    "\n",
    "\n",
    "\n",
    "### 为特征子空间选择线性判别式\n",
    "\n",
    "\n",
    "剩下的LDA步骤和PCA很像了。不同点是PCA分解协方差矩阵得到特征值和特征向量，LDA分解$S_{W}^{-1}S_{B}$得到特征值和特征向量：\n",
    "\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576d215eac03e.png)\n",
    "\n",
    "得到特征值后，对其降序排序：\n",
    "\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576d217898800.png)\n",
    "\n",
    "\n",
    "我们可以看到只有两个特征值不为0，其余11个特征值实质是0，这里只不过由于计算机浮点表示才不为0。极端情况是特征向量共线，此时协方差矩阵秩为1，只有一个非0特征值。\n",
    "\n",
    "\n",
    "为了度量线性判别式(特征向量)捕捉到了多少的类判别信息，我们画出类似方差解释率的线性判别图：\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576d238216c32.png)\n",
    "\n",
    "\n",
    "\n",
    "我们可以看到前两个线性判别捕捉到了Wine训练集中100%的有用信息：\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576d23d1144f3.png)\n",
    "\n",
    "然后由这两个线性判别式来创建转换矩阵$W$:\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576d245b7115d.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 原始数据映射到新特征空间\n",
    "\n",
    "有了转换矩阵$W$,我们就可以将原始数据映射到新的特征空间了：\n",
    "\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576d24cc6333f.png)\n",
    "\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576d2569395e9.png)\n",
    "\n",
    "新数据集，明显线性可分：\n",
    "\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576d259f4809a.png)\n",
    "\n",
    "\n",
    "### 调用sklearn中LDA\n",
    "\n",
    "我们通过一步步地实现LDA来加深理解，现在看看sklearn中如何使用现成的LDA：\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576d26907a38c.png)\n",
    "\n",
    "\n",
    "有一个样本被错分类：\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576d26b0af864.png)\n",
    "\n",
    "如果降低正则项的影响，完全正确分类训练集。当然了，过拟合并没有什么好处。我们看一下现在模型对测试集的分类效果：\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576d274c58876.png)\n",
    "\n",
    "Wow！100%的准确率。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用核PCA进行非线性映射\n",
    "\n",
    "许多机器学习算法都有一个假设：输入数据要是线性可分的。感知机算法必须针对完全线性可分数据才能收敛。考虑到噪音，Adalien、逻辑斯蒂回归和SVM并不会要求数据完全线性可分。\n",
    "\n",
    "但是现实生活中有大量的非线性数据，此时用于降维的线性转换手段比如PCA和LDA效果就不会太好。这一节我们学习PCA的核化版本，核PCA。这里的\"核\"与核SVM相近。 运用核PCA，我们能将非线性可分的数据转换到新的、低维度的特征子空间，然后运用线性分类器解决。\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576d29033ad87.png)\n",
    "\n",
    "### 核函数和核技巧\n",
    "\n",
    "还记得在核SVM那里，我们讲过解决非线性问题的手段是将他们映射到新的高维特征空间，此时数据在高维空间线性可分。为了将数据$x\\in R^{d}$映射到高维k空间，我们定义了**非线性映射**函数$\\phi$:\n",
    "\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576d29bda8386.png)\n",
    "\n",
    "\n",
    "\n",
    "我们可以把核函数的功能理解为：通过创造出原始特征的一些非线性组合，然后将原来的d维度数据集映射到k维度特征空间，d<k。举个例子，特征向量$x\\in R^{d}$，x是列向量包含d个特征，d=2，可以按照如下的规则将其映射到3维度特征空间：\n",
    "\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576d2ea26cc6f.png)\n",
    "\n",
    "同理核PCA的工作机制：通过核PCA的非线性映射，将数据转换到一个高维度空间，然后在这个高维度空间运用标准PCA重新将数据映射到一个比原来还低的空间，最后就可以用线性分类器解决问题了。不过，这种方法涉及到两次映射转换，计算成本非常高，由此引出了核技巧(kernel trick)。\n",
    "\n",
    "使用核技巧，我们能在原始特征空间直接计算两个高维特征向量的相似性(不需要先特征映射，再计算相似性)。\n",
    "\n",
    "\n",
    "在介绍核技巧前，我们先回顾标准PCA的做法。我们按照如下公式计算两个特征k和j的协方差：\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576d30ef6b5c0.png)\n",
    "\n",
    "由于我们对数据已做过标准化处理，特征平均值为0，上式等价于：\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576d31fecd692.png)\n",
    "\n",
    "\n",
    "同样，我们能够得到协方差矩阵：\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576d323bf3249.png)\n",
    "\n",
    "\n",
    "Bernhard Scholkopf(B. Scholkopf, A.Smola, and K.R. Muller. *Kernel Principal Component Analysis*. pages 583-588, 1997)得到了上式的泛化形式,用非线性特征组合$\\phi$替换原始数据集两个样本之间的点乘：\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576d3314adfe3.png)\n",
    "\n",
    "\n",
    "为了从协方差矩阵中得到特征向量(主成分)，我们必须求解下面的等式：\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576d348dedb77.png)\n",
    "\n",
    "\n",
    "其中，$\\lambda, v$是协方差矩阵$\\Sigma$的特征值和特征向量，$ \\bf a$的求法见下面几段内容。\n",
    "\n",
    "\n",
    "我们求解核矩阵：\n",
    "\n",
    "首先，我们写出协方差矩阵的矩阵形式，$\\phi(X)$是一个n*k的矩阵：\n",
    "\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576d368a81005.png)\n",
    "\n",
    "我们将特征向量写作：\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576d36b56873b.png)\n",
    "\n",
    "由于$\\Sigma \\bf v=\\lambda \\bf v$，得：\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576d37678ff0a.png)\n",
    "\n",
    "\n",
    "\n",
    "等式两边左乘$\\phi(\\bf X)$：\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576d37a83bcaf.png)\n",
    "\n",
    "\n",
    "\n",
    "这里的$\\bf K$就是相似性(核)矩阵：\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576d37e0aca23.png)\n",
    "\n",
    "\n",
    "\n",
    "回忆核SVM我们使用核技巧避免了直接计算$\\phi(x^{(i)})^{T}\\phi(x^{(j)})$：\n",
    "\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576de7a56dc96.png)\n",
    "\n",
    "核PCA同样不需要像标准PCA那样构建转换矩阵，我们使用核函数代替计算$\\phi(\\bf X)\\phi(\\bf X)^T$。所以，你可以把核函数(简称，核)理解为计算两个向量点乘的函数，结果可看做两个向量的相似度。\n",
    "\n",
    "\n",
    "\n",
    "常用的核函数有：\n",
    "\n",
    "\n",
    "* 多项式核：\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576de8773bb6c.png)\n",
    "，$\\theta$是阈值，$p$是由用户设定的指数。\n",
    "\n",
    "\n",
    "\n",
    "* 双曲正切(sigmoid)核：\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576de8da01a63.png)\n",
    "\n",
    "* 径向基函数核(高斯核)：\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576de91e7e9f7.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "现在总结一下核PCA的步骤，以RBF核为例：\n",
    "\n",
    "1 计算核(相似)矩阵k，也就是计算任意两个训练样本：\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576de97a39675.png)\n",
    "\n",
    "得到K:\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576de9c97416f.png)\n",
    "\n",
    "举个例子，如训练集有100个样本，则对称核矩阵K的维度是100*100。\n",
    "\n",
    "2 对核矩阵K进行中心化处理：\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576dea2e15413.png)\n",
    "\n",
    "其中,$1_{n}$是n*n的矩阵，n=训练集样本数，$1_{n}$中每个元素都等于$\\frac{1}{n}$.\n",
    "\n",
    "\n",
    "3 计算$\\bf K^{'}$的特征值，取最大的k个特征值对应的特征向量。不同于标准PCA，这里的特征向量并不是主成分轴。\n",
    "\n",
    "\n",
    "第2步为什么要计算$\\bf K^{'}$? 因为在PCA我们总是处理标准化的数据，也就是特征的平均值为0。当我们用非线性特征组合$\\phi$替代点乘时，我们并没有显示计算新的特征空间也就没有在新特征空间做标准化处理，我们不能保证新特征空间下的特征平均值为0，所以要对K做中心化。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用Python实现核PCA\n",
    "\n",
    "\n",
    "上一节我们讨论了核PCA的原理。现在我们根据上一节的三个步骤，自己实现一个核PCA。借助SciPy和NumPy，其实实现核PCA很简单：\n",
    "\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/24/576dee1658b5a.png)\n",
    "\n",
    "RBF核PCA的一个缺点是需要人工设置$\\gamma$值，调参不易。第六章我们会介绍调参技巧。\n",
    "\n",
    "\n",
    "\n",
    "### 例1 半月形数据分割\n",
    "\n",
    "现在我们创建一个非线性数据集试一下rbf_kernel_pca的效果，数据集100个样本，每个样本两维度特征，数据分布呈两个半月形：\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/25/576e36b4e77e4.png)\n",
    "\n",
    "\n",
    "每个半月数据是一类。\n",
    "\n",
    "显然图中的数据不是线性可分的，我们的目标是通过核PCA将\"曲线\"映射到\"直线\"上，然后就可以用线性分类器了。\n",
    "\n",
    "我们先看一下如果用标准PCA处理，会得到什么结果：\n",
    "\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/25/576e44a6a45dc.png)\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/25/576e44cbd28ac.png)\n",
    "\n",
    "很显然，用标准PCA降维后的数据无法用线性分类器处理。还要注意右边子图中的蓝线我们故意下调了一点点，红线上调了一点点，这是为了清楚地观察他们的重合部分。\n",
    "\n",
    "现在我们尝试rbf_kernel_pca，看看能不能解决非线性数据：\n",
    "\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/26/5770947b5b60d.png)\n",
    "\n",
    "\n",
    "我们可以看到经过核PCA处理后的数据线性可分了，所以可以直接用线性分类器来训练一个合适的模型。\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/26/5770949508c3b.png)\n",
    "\n",
    "RBF核PCA的缺点就是需要人工设定$\\gamma$，这个值不是凭空瞎想的，而是要做实验确定，在第六章，我们会讨论调参的技巧。\n",
    "\n",
    "\n",
    "\n",
    "### 例2 分离同心圆数据\n",
    "\n",
    "\n",
    "上一节我们创建了一个半圆形的数据，然后可以用核PCA线性分离，我们再来看另一个有趣的非线性数据集：\n",
    "\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/26/5770960725468.png)\n",
    "\n",
    "\n",
    "\n",
    "同样这也是一个二分类问题。我们分别用标准PCA和RBF核PCA处理数据集，然后比较他们的结果：\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/26/577096fe1b736.png)\n",
    "\n",
    "标准PCA效果不好，处理后不能应用线性分类器。\n",
    "\n",
    "再来看看RBF核PCA的效果能不能让我们满意：\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/26/577097857ffcc.png)\n",
    "\n",
    "Wow，结果非常好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 映射新的数据点\n",
    "\n",
    "在前面的两个例子中，我们将原始的数据集映射到新的特征空间。不过在实际应用中，我们常常需要将多个数据集转换，比如训练集和测试集，还有可能在训练好模型后，又收集到的新数据。在本节，你将学习如何将不属于训练集的数据进行映射。\n",
    "\n",
    "\n",
    "还记得在标准PCA中，我们通过计算 转换矩阵*输入样本，得到映射后的数据。转换矩阵的每一列是我们从协方差矩阵中得到的k个特征向量。现在，如何将这种思路应用到核PCA？在核PCA中，我们得到的特征向量来自归一化的核矩阵(centered kernel matrix)，而不是协方差矩阵，这意味着样本已经被映射到主成分轴$v$.因此，如果我们要把一个新样本$\\bf x^{'}$ 映射到主成分轴，我们要按照下式:\n",
    "\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/27/57711fdcd091a.png)\n",
    "\n",
    "上式怎么算？当然不好算，好在我们还有核技巧，所以可以避免直接计算$\\phi(x^{'})^{T}v$。\n",
    "\n",
    "和标准PCA不同的是，核PCA是一种基于内存的方法，这是什么意思呢？意思是每次对新样本进行映射时就要用到所有的训练集。因为要计算训练集中每个样本和新样本$x^{'}$之间的RBF核(相似度):\n",
    "\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/27/577120ee9abbe.png)\n",
    "\n",
    "其中，核矩阵$\\bf K$的特征向量$\\bf a$和特征值$\\lambda$满足条件: $\\bf K\\bf a=\\lambda \\bf a$。\n",
    "\n",
    "计算每一个训练集样本和新样本的$k()$后，我们必须用特征值对特征向量做归一化。所以呢，我们要修改一下前面实现的RBF PCA，能够返回核矩阵的特征向量：\n",
    "\n",
    "```\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy import exp\n",
    "from scipy.linalg import eigh\n",
    "import numpy as np\n",
    "\n",
    "def rbf_kernel_pca(X, gamma, n_components):\n",
    "\"\"\"\n",
    "RBF kernel PCA implementation.\n",
    "\"\"\"\n",
    "# Calculate pairwise squared Eculidean distances\n",
    "sq_dists = pdist(X, 'sqeuclidean')\n",
    "mat_sq_dists = squareform(sq_dists)\n",
    "#Compute the symmetric kernel matrix\n",
    "K = exp(-gamma * mat_sq_dists)\n",
    "#Center the kernle matrix\n",
    "N = K.shape[0]\n",
    "one_n = np.ones((N, N)) / N\n",
    "K = K - one_n.dot(K) - K.dot(one_n) + one_n.dot(K).dot(one_n)\n",
    "#Obtaining eigenpairs from the centered kernel matrix\n",
    "eigvals, eigvecs = eigh(K)\n",
    "alphas = np.column_stack((eigvecs[:, -i]\n",
    "for i in range(1,n_components+1)))\n",
    "#Collect the corresponding eigenvalues\n",
    "lambdas = [eigvals[-i] for i in range(1, n_components+1)]\n",
    "return alphas, lambdas\n",
    "\n",
    "```\n",
    "\n",
    "现在，我们创建一个新的半月形数据集，然后用更新过的核PCA将其映射到一维子空间：\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/27/577122acf1fb8.png)\n",
    "\n",
    "\n",
    "\n",
    "为了检验对于新数据点的映射表现，我们假设第26个点时新数据点$x^{'}$，我们的目标就是将这个新数据点进行映射:\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/27/57712436e1f01.png)\n",
    "\n",
    "使用project_x函数，我们能够对新数据样本进行映射：\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/27/57712493920e0.png)\n",
    "\n",
    "\n",
    "\n",
    "最后，我们将第一主成分进行可视化：\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/27/577124bfcd88f.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn中的核PCA\n",
    "\n",
    "sklearn.decomposition中有核PCA的实现，看看怎么用：\n",
    "\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/27/5771260d29fc3.png)\n",
    "\n",
    "通过kernel参数设定不同的核函数。\n",
    "\n",
    "将转换后的数据可视化：\n",
    "\n",
    "\n",
    "\n",
    "![](https://ooo.0o0.ooo/2016/06/27/577125f77dcf5.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "本章，你学习了三种基本的用于特征抽取的降维方法：标准PCA、LDA和核PCA。\n",
    "\n",
    "使用PCA，我们将数据映射到一个低维度的子空间并且最大化正交特征轴的方差，PCA不考虑类别信息。LDA是一种监督降维方法，意味着他要考虑训练集的类别信息，目标是将类别最大化地可分。最后，你学习了核PCA，它能够将非线性数据集映射到低维特征空间，然后数据变成线性可分了。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
